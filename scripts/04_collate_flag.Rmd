---
title: "Collate, flag and finalize"
author: "Sam Struthers"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}

source("scripts/00_setup.R")
```

# Import Datasets
- Cap rod
- DO
- ISCO

## Cap Rod

### Load Functions
```{r}
# source all functions in the cap_rod_functions folder
walk(list.files(path = "scripts/01_cap_rod_functions/", full.names = TRUE), source)

```

### Import datasets

```{r}


##### ----- Read in 2022 prepped Data ------ ######


cap_rod_2022 <- read_csv("data/raw/cap_rod/2022/stage_combined_UNCLEAN.csv", show_col_types = FALSE)%>%
  mutate(DT = with_tz(DT, tzone = "MST"), 
         time = format(DT, "%H:%M"))


### ----- Read in 2023 Data ------ ###### 

cap_rod_2023 <- import_cap_rod(folder_path = "data/raw/cap_rod/")%>%
  mutate(date = as_date(DT))
# combine 2022 and 2023 into one df

#stage_df <- bind_rows(cap_rod_2022, cap_rod_2023)



```

### Read in manual measurements

```{r}
##  2023 Field  Discharge measurements ##
 Q_2023 <- calc_manual_q(folder_path = "data/raw/manual_Q/")%>%
  select(site,date, q_cfs)
#ToDo: left join with manual stage measurements by date and site


##  2022 Field  Discharge and stage measurements ##
manual_measurements <- read_csv("data/raw/manual_stage.csv", show_col_types = FALSE)%>%
  mutate(DT = as.POSIXct(DT, format = "%m/%d/%y %H:%M"), 
         date = as_date(DT))


```

### Adjust stage data
There are no stage measurements for 2023, so we will use the 2022 measurements to adjust the 2022 stage data. 2023 data will be joined later and will not be adjusted.

```{r}

#*Using some driftR functions and pmap, we can map over the manual measurements df and preform the adjustments.*

# fit column names to match argument names in functions
# Since there are no manual stage measurements for 2023, we can just use the 2022 measurements

manual_measurements_for_func <- manual_measurements %>% 
  filter(year(DT) == 2022) %>%
  format_manual_stage()%>%
select(site_name = site_code, 
                start_date = first_meas_DT,
                end_date = last_meas_DT,
                manual_start = first_meas_cm, 
                manual_end = last_meas_cm) %>%
  # NA values will break the functions, get rid of em!
  na.omit() %>%
  # pmap_dfr needs tibbles :)
  as_tibble()

##PMAP FOR ADJUSTMENT FUNCTIONS!##
#
stage_df <- cap_rod_2022

# steps df for each subset between site visits to match the start measurement
stepped_df <- pmap_dfr(manual_measurements_for_func, adjust_stage)

# creates a time factor for each subset using the `driftR` factor function
factor_df <- pmap_dfr(manual_measurements_for_func, dr_factor_subset)

# using factor and last 4 measurements from manual end, corrects for drift so that the manual end and the end measurement match up
corrected_df <- pmap_dfr(manual_measurements_for_func, dr_correctOne_subset)%>%
  mutate(corrected_stage_cm = round(corrected_stage_cm, digits = 1) )

# remove extra datasets from environment
rm(manual_measurements_for_func, stepped_df, factor_df)


```

### Adjusting 2023 SAWM data

```{r}

#*Using some driftR functions and pmap, we can map over the manual measurements df and preform the adjustments.*

# fit column names to match argument names in functions
manual_measurements_for_func <- manual_measurements %>% 
  filter(year(DT) == 2023) %>%
  format_manual_stage()%>%
select(site_name = site_code, 
                start_date = first_meas_DT,
                end_date = last_meas_DT,
                manual_start = first_meas_cm, 
                manual_end = last_meas_cm) %>%
  # NA values will break the functions, get rid of em!
  na.omit() %>%
  # pmap_dfr needs tibbles :)
  as_tibble()

##PMAP FOR ADJUSTMENT FUNCTIONS!##
#
stage_df <- cap_rod_2023%>%
  filter(site_code == "SAWM")

# steps df for each subset between site visits to match the start measurement
stepped_df <- pmap_dfr(manual_measurements_for_func, adjust_stage)

# creates a time factor for each subset using the `driftR` factor function
factor_df <- pmap_dfr(manual_measurements_for_func, dr_factor_subset)

# using factor and last 4 measurements from manual end, corrects for drift so that the manual end and the end measurement match up
corrected_df_2023 <- pmap_dfr(manual_measurements_for_func, dr_correctOne_subset)%>%
  mutate(corrected_stage_cm = round(corrected_stage_cm, digits = 1) )

cap_rod_non_adjusted <- cap_rod_2023%>%
  filter(site_code != "SAWM" | DT >= max(manual_measurements_for_func$end_date))

# remove extra datasets from environment
rm(manual_measurements_for_func, stepped_df, factor_df)


```



### Join in 2023 stage data

```{r}


# join in 2023 stage data 
corrected_df <- corrected_df %>%
  bind_rows(corrected_df_2023) %>%
  bind_rows(cap_rod_non_adjusted)

```



### Splitting sites where caprod was moved

*LBEA and SHEP had to be moved mid-season in 2022 so they are now being split into LBEA 1/2 and SHEP 1/2*



```{r site_split}

# This dataframe contains the old names, new names and start/end dates for the name changes
rename_df <- data.frame(old_name = c("LBEA", "SHEP"), 
                       change_date = c("2022-08-19 10:15","2022-08-19 12:30"))%>% 
  mutate(change_date = as.POSIXct(change_date, format = "%Y-%m-%d %H:%M", tz = "MST"))

##################
# If there are more moves in the future, this can be written in as a csv with the same  column names and read in using the hashed out code below:

# rename_df <- read.csv("FILE_PATH_TO.CSV") %>% 
#mutate(change_date = as.POSIXct(change_date, format = "%Y-%m-%d %H:%M", tz = "MST"))
##################

# function to change site_code to new names in rename_df if between start/end dates
change_site_names <- function(old_name, change_date) {

  corrected_df %>%
    mutate(site_code = case_when(
      site_code == old_name & DT < change_date ~ paste0(old_name, "1"),
      site_code == old_name & DT >= change_date ~ paste0(old_name, "2"),
      TRUE ~ site_code
    ))
  
}
# change site names
final_stage_df <- pmap_dfr(rename_df, change_site_names)%>%
  filter(site_code %nin% rename_df$old_name)%>%
  distinct()%>%
  # remove extra columns and rename for final variables
  select(DT, date,
         site_code, 
         sensor_stage_cm = water_height_cm, 
         adjustment_cm = adjustment,
         adjusted_stage_cm = adjusted_stage,
         time_correction_value = corrFac, 
         corrected_stage_cm,
         time)
# check that all the new names are in the df and old names are removed
unique(final_stage_df$site_code)
# if old names persist you may need to change the start/end dates so that all rows are included

#remove extra vars from environment
rm(rename_df, change_site_names)

```


### Create rating curves

```{r}

#Import csv with measurement DT, stage H and measured Q
h_q_df <- manual_measurements %>%
  filter(!is.na(q_cfs))%>%
  mutate(Year = as.character(year(DT)), 
          site_code = case_when(site_code == "LBEA"& DT < "2022-08-19 10:15" ~ "LBEA1", 
                                site_code == "LBEA"& DT >= "2022-08-19 10:15" ~ "LBEA2", 
         TRUE ~ site_code)
         )%>%
  select(H_cm,q_cfs,site_code, Year, DT)

# list sites that we want to calculate rating curve for
site_list <- unique(h_q_df$site_code)

# using manual dataframe to create coefficients for each site
rating_curve_coeffs <- map_dfr(site_list, create_rating_curve)
```


### Create 15 min Q timeseries
Based on stage data and rating curves

```{r}
## *15min Q calculations*
# calculate bounded Q for 15min data for all sites in site_list
stage_Q_15min <- map_dfr(site_list, calc_Q_for_sensor_stage_15min)

```
### Create Daily Mean Q timeseries

```{r}
# take mean for each created stage value
daily_means_stage <- final_stage_df %>%
  dplyr::select(sensor_stage_cm, corrected_stage_cm, adjusted_stage_cm, date, site_code) %>%
  group_by(date, site_code) %>%
  dplyr:: summarise(mean_sensor_stage_cm = mean(sensor_stage_cm, na.rm = TRUE),
                    mean_adjusted_stage_cm = mean(adjusted_stage_cm, na.rm = TRUE),
                    mean_corrected_stage_cm = mean(corrected_stage_cm, na.rm = TRUE)) %>%
  ungroup()


## *Daily Q calculations*
# calculate bounded Q for daily data for all sites in site_list
stage_Q_daily <- map_dfr(site_list, calc_Q_for_sensor_stage_daily)%>%
  rename(site = site_code)

```

### Add flags to data

```{r}
## ------- 15 minute flagging ------- ##

# flagging_csv <- "data/flag_values_caprods_2023.csv"
# 
# ## ------ Daily flagging ------ ##
# 
# # create df of sites and their codes for data flag, but on a daily timescale
# flag_daily_values<- flag_values %>%
#   mutate(start_date = as.Date(start_dt), 
#          end_date = as.Date(end_dt)) %>%
#   select(site, FLAG, start_date, end_date, notes)
# 
# ##### Daily flag function in `apply_daily_flag.R`
# 
# # this may induce duplicates!
# correct_and_flagged_daily_df <- pmap_dfr(flag_daily_values, apply_flag_daily)

```

### Export collated and adjusted data

```{r}
folder <- "data/FINAL/"

write_csv(stage_Q_15min, 
          paste0(folder, "stage_Q_15min_CPF.csv"))

write_csv(stage_Q_daily, 
          paste0(folder, "stage_Q_daily_CPF.csv"))

all_stage_Q_15min <- stage_Q_15min %>%
  rename(site = site_code, DT_round_mst = DT)%>%
  mutate(site = case_when(site == "LBEA1" ~ "LBEA",
                          site == "LBEA2" ~ "LBEA",
                          TRUE ~ site))%>%
  select(site, DT_round_mst, Q_cfs,sensor_stage_cm, adjustment_cm, corrected_stage_cm, extrapolated_Q_cfs, q_flag)
```

### Clean up

```{r}
#Remove extra datasets from environment
rm(cap_rod_2022, cap_rod_2023, h_q_df, daily_means_stage, stage_df, corrected_df)
```
## Visualize

```{r}
# graph Q for sawm using stage_q_daily facet wrap for 2022 and 2023
# ggplot(filter(stage_Q_daily, site_code == "SAWM"), aes(date, Q_cfs))+
#   geom_point(aes(color = q_flag))+
#    # geom_point(data = filter(manual_measurements, site_code == "SAWM"), aes(x= date, y = q_cfs))+
#   #geom_point(data = filter(Q_2023, site == "FISH"), aes(x= date, y = q_cfs), color = "blue")+
#   facet_wrap(~year(date), scales = "free_x", ncol = 1, nrow = 2)+
#   theme_bw()+
#   theme(legend.position = "right")+
#   labs(x = "Date", y = "Discharge (cfs)", color = "Q Calc Flag")

#ggsave("data/figs/SAWM_daily_Q_2022_2023.png", width = 10, height = 5, dpi = 300)

# ggplot(filter(stage_Q_15min, site_code == "SAWM" & year(DT) == 2022 ), aes(DT))+
#   geom_point(aes(y = sensor_stage_cm))+
#    geom_point(aes(y = corrected_stage_cm), color = "red")+
#    geom_point(data = filter(manual_measurements, site_code == "SAWM"& year(DT) == 2022  ), aes(y = H_cm), color = "blue")

# testing <- filter(stage_Q_daily, site_code == "SAWM")%>%
#   mutate(year = as.character(year(date)), 
#          doy = yday(date))
# 
# ggplot(testing, aes(doy, Q_cfs))+
#   geom_point(aes(color = year))

```


## ISCO
### Read in data
```{r}
source("scripts/02_ISCO_binder.R")

# THIS WILL BREAK IF YOU HAVE A FILE OPEN in excel
#Reading in collated 2023 data
raw_isco_xlsx_files <- list.files(
  path = "data/raw/ISCO/", # change to folder path of collated ISCO files
  full.names = TRUE, pattern = ".xlsx")
# read in all isco files
isco_2023 <- map_dfr(raw_isco_xlsx_files, read_isco_file) 

# change to folder path of .dat files
raw_dat_files <- list.files(
  path = "data/raw/ISCO/",   # change to folder path of raw ISCO .dat files
  pattern = ".dat", full.names = TRUE, recursive = TRUE)

#collate and organize all raw data from .dat files
# This is relatively slow (~30 sec per file) so if you are parsing alot of files it may take a few minutes
#isco_2022 <- clean_collate_dat_files(dat_files = raw_dat_files)
# #save collated data
#  write_csv(isco_2022, "data/collated/isco_data_2022.csv")
# write_rds(isco_2022, "data/collated/isco_data_2022.RDS")
# Once it has been run (assuming no new data), you can just load the RDS file for faster opening
isco_2022 <- readRDS(file ="data/collated/isco_data_2022.RDS" )

#combine both user collated and raw .dat files
all_isco <- rbind(isco_2023, isco_2022)%>%
  mutate(turb_rounded = round(turb, digits = 1))%>%
  select(-record)
#remove file path vectors
rm(raw_isco_xlsx_files, raw_dat_files, isco_2022, isco_2023)

```
### Visualize

```{r}
isco_long <- all_isco%>%
  pivot_longer(cols = c(ISCO_temp_c,sc_us, cdom_ave, cdom, turb_ave, turb), names_to = "parameter", values_to = "value")

# plot <- ggplot(all_isco, aes(x= DT_round_mst, y= ISCO_temp_c, color= site))+
#   geom_line()+
#   facet_wrap(~year(DT_round_mst), nrow = 2, scales = "free_x")
# 
# ggplotly(plot)
```


## DO
### Read in data
```{r}
#source script with DO binder function
source("scripts/03_miniDOT_binder.R")

#Raw DO folder paths
raw_do_dir <- list.files(path = "data/raw/DO", full.names = TRUE)
# Run all filepaths thru dot puller
all_do <- map_dfr(.x = raw_do_dir, .f = dot_puller)
```
### Visualize DO

```{r}
# DO_plot <- ggplot(all_do, aes(x= DT_round_mst, y= DO_mgL, color= site))+
#   geom_line()+
#   geom_smooth()
# 
# ggplotly(DO_plot)
```
## Precip

Pulled from Larimer County website in `scripts/q_precip_grab.rmd`

```{r}

precip <- readRDS("data/larimer_co_precip_2021_2023.rds")
```

# Collate all data

```{r}

# List all sites to be processed below
sites <- c("SAWM","FISH", "LBEA", "BLAK" )
start_dt <- "2022-01-01 00:00:00"
end_dt <- "2023-12-31 23:55:00"

create_all_data <- function(site_name){
start_date <- as.POSIXct(start_dt, tz = "MST")
end_date <- as.POSIXct(end_dt, tz = "MST")

site_data <- tibble(
  site = site_name,
  DT_round_mst = seq(start_date, end_date, by = "5 min"))

return(site_data)
}

all_data <- map_dfr(sites, create_all_data)%>%
  left_join(all_isco, by = c("site", "DT_round_mst"))%>%
  left_join(all_do, by = c("site", "DT_round_mst"))%>%
  left_join(all_stage_Q_15min, by = c("site", "DT_round_mst"))%>%
  left_join(precip, by = "DT_round_mst")%>%
  mutate(precip_in = case_when(site %in% c("FISH", "LBEA", "BLAK") ~ `Larimer - Salt Cabin`, 
                               #site %in% c("LBEA","FISH") ~ `Larimer - Old Flowers Road`,
                               site == "SAWM"  ~ `Larimer - Joe Wright Reservoir`, 
         TRUE ~ NA), 
         precip_in = case_when(precip_in == 0 ~ NA,TRUE ~ precip_in),
         precip_site = case_when(site %in% c("FISH", "LBEA", "BLAK") ~ "Larimer - Salt Cabin", 
                               #site %in% c("LBEA","FISH") ~ `Larimer - Old Flowers Road`,
                               site == "SAWM"  ~ "Larimer - Joe Wright Reservoir", 
         TRUE ~ NA)
         )%>%
  select(-`Larimer - Salt Cabin`, -`Larimer - Old Flowers Road`, -`Larimer - Joe Wright Reservoir`,
         -`Larimer - Roaring Creek at Hwy 14`, -`Larimer - Tunnel Creek at Hwy 14`)%>%
  
  mutate(date = as.Date(DT_round_mst))%>%
  #filter(rowSums(!is.na(select(., -site, -DT_round_mst, -precip_site,-date ))) > 0)%>%
  select(site, DT_round_mst, date, precip_site, q_flag, everything())
rm(all_isco, all_do, all_stage_Q_15min, DO_plot)
```
## Daily Conversion
 Convert the entire dataframe to daily values
```{r}

all_data_daily <- all_data%>%
  select(-q_flag, -Q_cfs, -sensor_stage_cm, -adjustment_cm, -corrected_stage_cm, -extrapolated_Q_cfs)%>%
  group_by(date, site)%>%
  summarise(across(sc_us:DO_mgL, ~ mean(.x, na.rm = TRUE)), 
            sum_precip_in = sum(precip_in, na.rm = TRUE))%>%
  ungroup()%>%
  left_join(select(stage_Q_daily, -a, -b, -r2, -year), by = c("date", "site"))%>%
  mutate(sum_precip_in = case_when(sum_precip_in == 0 ~ NA, TRUE ~ sum_precip_in),
    across(sc_us:extrapolated_Q_cfs, ~na_if(., NaN)))%>%
 filter(rowSums(!is.na(select(., -site,-date ))) > 0)%>%
  mutate(doy = yday(date), 
         year = as.character(year(date)))
  


all_data <- all_data%>%
  filter(rowSums(!is.na(select(., -site, -DT_round_mst, -precip_site,-date ))) > 0)
```

### Long format
```{r}
all_data_long <- all_data%>%
  pivot_longer(cols = c(sc_us:precip_in), names_to = "parameter", values_to = "value")%>%
  filter(!is.nan(value) & !is.na(value))

all_data_daily_long <- all_data_daily%>%
  select(date, site, q_flag, doy, year,everything() )%>%
  pivot_longer(cols = c(sc_us:extrapolated_Q_cfs), names_to = "parameter", values_to = "value")%>%
  filter(!is.nan(value) & !is.na(value))

```


## Plotting results
```{r}
# # View fish temp data between sensors
# ggplot(filter(all_data_daily, site == "FISH"), aes(x = doy)) +
#    geom_point(aes(y = minidot_temp_c, color = "Minidot Temp", shape = year), size = 3) +
#    geom_point(aes(y = ISCO_temp_c, color = "ISCO Temp", shape = year), size = 3) +
#   # geom_smooth(aes(y = mean_minidot_temp_c, color = "Minidot Temp"), se = FALSE)+
#   # geom_smooth(aes(y = mean_ISCO_temp_c, color = year), se = FALSE)+
#   labs(x = "Date",
#        y = "Mean temp") +
#   scale_color_manual(values = c("Minidot Temp" = "red", "ISCO Temp" = "blue", "2022" = "green", "2023" = "purple")) +
#   theme_minimal()
# +
#   facet_wrap(~site)
# 
# # view all data all sensors at 5 min intervals
 ggplot(data = filter(all_data_long, year(date) == "2022"&parameter %in% c("precip_in", "turb_ave",  "Q_cfs")& site == "SAWM"), aes(x= DT_round_mst, y = value, color = site))+
   geom_point()+
   facet_wrap(~parameter, scales = "free_y", ncol = 1, nrow = 3)
# 
# # view daily data
# ggplot(data = filter(all_data_daily, site == "SAWM"), aes(x= date, y = Q_cfs, color = q_flag))+
#   geom_point()+ 
#   facet_wrap(~year, scales = "free_x", ncol = 1, nrow = 2)


```

## Exporting

```{r}
dir = "data/final"
write_csv(all_data, file.path(dir, "all_data.csv"))
write_rds(all_data, file.path(dir, "all_data.rds"))
write_csv(all_data_daily, file.path(dir, "all_data_daily.csv"))
write_rds(all_data_daily, file.path(dir, "all_data_daily.rds"))
write_csv(all_data_long, file.path(dir, "all_data_long.csv"))
write_rds(all_data_long, file.path(dir, "all_data_long.rds"))
write_csv(all_data_daily_long, file.path(dir, "all_data_daily_long.csv"))
write_rds(all_data_daily_long, file.path(dir, "all_data_daily_long.rds"))


```


# Flagging


Flags were determined visually and compared to field notes about the streams.

Flag names:

PASS: Data looks good

FAIL: Data is questionable, caution using it

CHANNEL: Channel changes occurred, stage may not be accurate

TREND: Magnitude of data is correct but data trends un-intuitively

VARIATION: Daily variation is questionable, use larger time step for safety

Any additional: Explained in notes

## Adding flags to data
```{r}

# 4 columns:
# start_dt <- when flag starts
# end_dt <- when flag ends
      ## make sure start != end, will duplicate data
# FLAG: flag code
# notes: any notes about the flag code or field notes
flag_values<- read_csv("data/flag_values_caprods_2023.csv", show_col_types = FALSE)%>%
  mutate(start_dt = as.POSIXct(start_dt, format = "%m/%d/%y %H:%M", tz = "MST"),
         end_dt = as.POSIXct(end_dt, format = "%m/%d/%y %H:%M", tz = "MST"))%>%
  select(site = site_code, start_dt, end_dt, FLAG, notes = Notes)

#DEV needed
correct_and_flagged_df <- apply_flag_15min()


```

## Viewing Flags
```{r}
# colors to visualize site flags in plots
flag_colors <- c("PASS" = "green", 
                 "FAIL" = "red", 
                 "TREND" = "purple",
                 "VARIATION"= "blue",
                 "CHANNEL"= "orange") 
```



