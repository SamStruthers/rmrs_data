---
title: "Caprod Cleaning"
format: html
editor: visual
author: "Sam Struthers"
author-meta: "Sam Struthers"
---

## *Packages*

```{r setup}
#setwd("/Users/samstruthers/Documents/fork_yeah/rmrs_data")
#change to whatever your working directory is 
library(knitr)
opts_knit$set(root.dir = "~/fork_yeah/rmrs_data/")
source("scripts/00_setup.R")
walk(list.files(path = "scripts/cap_rod_functions/", full.names = TRUE), source)
```

## *Importing and Combining*

*RULES:*

1.  *All data must be in CSVs nested in the folders data/all_sites/*

2.  *All files must have naming convention: SITE\_#DATE#*

3.  *All files must have same number of columns and column names*

4.  *All files must have the same DateTime format (can be adjusted below)*

5.  *Run all_data first to get what the pattern for gsub will be*

    1.  *This is also a good way to test that none of your files are broken*

6.  *Use plots to double check everything*

    1.  *Use the plots to double check there are no zeros near site visits. If there are, remove them from the dataset manually! This will help the adjustments run correctly.*

7.  *Manual Measurements need the columns:*

    1.  *DT: site visit datetime*

    2.  *H_cm: site visit stage measurement*

        1.  This is a absolute value, if sediment has filled in 10 cm and stage plate is reading 20, you use 10 as the H_cm value

    3.  Q_cfs: Computed discharge

    4.  *optional columns : notes*

        1.  *Q_cfs is read in a different chunk*

```{r import_sensor_data}
# location of raw data files
folder_path <- "data/raw/cap_rod/"


# The function `import_cap_rod.R`, given a folder_path, will collate all stage data into a single data frame
# Make sure each file starts with the site name and is proceeded by a underscore
# Excel likes to mess with datetimes so check the data in excel first, if needing to, correcting the date time to the format: m/d/yy HH:MM
#Also make sure all the datasets have the same datetime column and set it below in date_time_col
date_time_col <- "correct_time"

# map over the folder path
stage_df <- import_cap_rod(folder_path = folder_path)


```

## Importing manually recorded values

```{r import_manual_data}

  
##  FIELD STAGE MEASUREMENTS. SEE RULES FOR COLUMNS NAMES ##
# change "field stage" to wherever manual values are stored:
field_stage <- "data/raw/manual_stage_2023.xlsx"

manual_measurements <- read_xlsx(field_stage)%>%
  format_manual_stage()

old_measurements <- "manual_values_dec_2022"

# Join both? or just bring it when calculating rating curves


```

## *Visual*

-   *Check for missing data, weird dates, see if there are any 0s near site visits.*

```{r pre_clean_visual}


master_visual <- stage_df %>%
  ggplot(aes(x = DT, y = water_height_cm_inst, color = site_code)) +
  geom_line()+
  # geom_point(data = manual_meass, aes(x = first_meas_DT, 
  #                                                 y = first_meas_cm, color = site_code)) +
  theme_bw()
ggplotly(master_visual)

indv_visual <- filter(stage_df, site_code == "SAWM") %>%
  ggplot(aes(x = DT, y = water_height_cm_inst, color = site_code)) +
  geom_line()+
  geom_point(data = filter(manual_measurements, site_code == "SAWM"), aes(x = first_meas_DT, 
                                                  y = first_meas_cm),
             color = "blue") +
  theme_bw()
   
ggplotly(indv_visual)
```

## *Export uncleaned stage across all sites*

```{r collated_export}
write_csv(stage_df, "data/collated/stage_UNCLEAN.csv")
```

# *Field data adjustments*

*In broad strokes, these functions/loops will take the sensor data pulled in above and manual values measured in the field to correct sensor data to match field data. Workflow overview:*

1.  *Subset the sensor data frame by each field measurement (i.e., each subset is bounded by two field measurements).*
2.  *Step sensor reading up or down based on the difference between first field measurement and the sensor value.*
3.  *Create a new column, time factor, for each subset of data which allows the `driftR` package to correct for sensor drift.*
4.  *Use `driftR` functions to match up the end of the data set with last field measurement.*



## *Functions to subset and adjust stage values*

```{r subset_adjust_functions}
# functions in subset_and_adjust.R
```

## *Preforming the adjustments*

*Using the functions written above and pmap, we can map over the manual measurements df and preform the adjustments.*

```{r adjusting}
# fit column names to match argument names in functions
manual_measurements_for_func <- manual_measurements %>% 
  dplyr::select(site_name = site_code, 
         start_date = first_meas_DT,
         end_date = last_meas_DT,
         manual_start = first_meas_cm, 
         manual_end = last_meas_cm) %>%
  # NA values will break the functions, get rid of em!
  na.omit() %>%
  # pmap_dfr needs tibbles :)
  as_tibble()

##PMAP FOR ADJUSTMENT FUNCTIONS!##

# steps df for each subset between site visits to match the start measurement
stepped_df <- pmap_dfr(manual_measurements_for_func, adjust_stage)

# creates a time factor for each subset using the `driftR` factor function
factor_df <- pmap_dfr(manual_measurements_for_func, dr_factor_subset)

# using factor and last 4 measurements from manual end, corrects for drift so that the manual end and the end measurement match up
corrected_df <- pmap_dfr(manual_measurements_for_func, dr_correctOne_subset)
```

### *Graphing to check work*

```{r check_adjust}
# change site names to whatever site you want to look at
indv_corrected_df <- filter(corrected_df, 
                            site_code == "SAWM")

indv_manual_measurements <- filter(manual_measurements_for_func, 
                                   site_name == "SAWM")
 
  
 indv_corrected_graph <- ggplot() +
  geom_line(data = indv_corrected_df, aes(x = DT , y = water_height_cm_inst), 
            color = "red") +
  geom_line(data = indv_corrected_df, aes(x = DT , y = corrected_stage_cm), 
            color = "black") +
  geom_line(data = indv_corrected_df, aes(x = DT , y = adjusted_stage), 
            color = "green", alpha = .5) +
  geom_point(data = indv_manual_measurements, aes(x = start_date, 
                                                  y = manual_start),
             color = "blue") +
   theme_bw()
 
ggplotly(indv_corrected_graph)
# ^Interactive graph^
 
plot(indv_corrected_graph)


```


## *Splitting sites where caprod was moved*

*LBEA and SHEP had to be moved mid-season so they are now being split into LBEA 1/2 and SHEP 1/2*

*This section can be skipped if sensors do not move locations.*

```{r site_split}

# This dataframe contains the old names, new names and start/end dates for the name changes
rename_df <- data.frame(old_name = c("LBEA", "LBEA", "SHEP","SHEP"), 
                       new_name = c("LBEA1", "LBEA2", "SHEP1", "SHEP2"), 
                       start_date = c("2022-05-01 12:00", "2022-08-19 10:15", 
                                      "2022-05-01 12:00", "2022-08-19 12:30"), 
                       end_date= c("2022-08-19 10:00", "2022-11-01 12:00",
                                   "2022-08-17 15:15", "2022-11-01 12:00")) %>% mutate(start_date = as.POSIXct(start_date, format = "%Y-%m-%d %H:%M", tz = "MST"),
       end_date = as.POSIXct(end_date, format = "%Y-%m-%d %H:%M", tz = "MST"))

##################
# If there are more moves in the future, this can be written in as a csv with the same  column names and read in using the hashed out code below:


# rename_df <- read.csv("FILE_PATH_TO.CSV") %>% 
#mutate(start_date = as.POSIXct(start_date, format = "%Y-%m-%d %H:%M", tz = "MST"),
#       end_date = as.POSIXct(end_date, format = "%Y-%m-%d %H:%M", tz = "MST"))
##################

# function to change site_code to new names in rename_df if between start/end dates
change_site_names <- function(old_name, new_name, start_date, end_date) {
  
  test_function <- corrected_df %>%
    dplyr:: filter(site_code == old_name & between(DT, start_date, end_date)) %>%
    mutate(site_code = new_name)
  
  return(test_function)
  
}

# rest of the sites with no changes to location
non_name_changed_df = corrected_df[!(corrected_df$site_code %in% 
                                       unique(rename_df$old_name)),]

# change site names
corrected_df<- pmap_dfr(rename_df, change_site_names) %>%
  # combine with unchanged df
  rbind(non_name_changed_df)

# check that all the new names are in the df and old names are removed
unique(corrected_df$site_code)

# if old names persist you may need to change the start/end dates so that all rows are included
```

## *Adding flags to 15 min*

*Flags were determined visually and compared to field notes about the streams.*

*Flag names:*

-   *PASS: Data looks good*

-   *FAIL: Data is questionable, caution using it*

-   *CHANNEL: Channel changes occurred, stage may not be accurate*

-   *TREND: Magnitude of data is correct but data trends un-intuitively*

-   *VARIATION: Daily variation is questionable, use larger time step for safety*

-   *Any additional: Explained in notes*

```{r flagging}
# 4 columns:
# start_dt <- when flag starts
# end_dt <- when flag ends
      ## make sure start != end, will duplicate data
# FLAG: flag code
# notes: any notes about the flag code or field notes
flag_values<- read_csv("data/flag_values_caprods_2023.csv", show_col_types = FALSE)%>%
  mutate(start_dt = as.POSIXct(start_dt, format = "%m/%d/%y %H:%M", tz = "MST"),
         end_dt = as.POSIXct(end_dt, format = "%m/%d/%y %H:%M", tz = "MST"))%>%
  select(site = site_code, start_dt, end_dt, FLAG, notes = Notes)


# apply flag in add flags.R
#map over flag values df
correct_and_flagged_df <- pmap_dfr(flag_values, apply_flag_15min)

# colors to visualize site flags in plots
flag_colors <- c("PASS" = "green", 
                 "FAIL" = "red", 
                 "TREND" = "purple",
                 "VARIATION"= "blue",
                 "CHANNEL"= "orange") 

indv_site <- correct_and_flagged_df %>%
  filter(site_code == "SAWM")

indv_site_flags <- ggplot() +
  geom_point(data = indv_site, aes(x = DT, y = corrected_stage_cm,
                                   color = flag_type )) +
  scale_color_manual(values = flag_colors) +
  theme_bw()
                     
plot(indv_site_flags)                     
```

### *Exporting 15 min stage data*

```{r export_15}
final_stage_df<- correct_and_flagged_df %>%
  select(DT, 
         site_code, 
         sensor_stage_cm = water_height_cm_inst, 
         adjustment_cm = adjustment,
         adjusted_stage_cm = adjusted_stage,
         time_correction_value = corrFac, 
         corrected_stage_cm,
         flag_type, 
         notes = note,
         date, 
         time)

write.csv(final_stage_df,"data/final/corrected_15min_stage_2023.csv")

```

## *Computing daily averages*

```{r daily_mean}
# take mean for each created stage value
daily_means_stage <- corrected_df %>%
  dplyr::select(water_height_cm_inst, corrected_stage_cm, adjusted_stage, date, site_code) %>%
  group_by(date, site_code) %>%
  dplyr:: summarise(mean_sensor_stage_cm = mean(water_height_cm_inst),
         mean_adjusted_stage_cm = mean(adjusted_stage),
         mean_corrected_stage_cm = mean(corrected_stage_cm)) %>%
  ungroup()
```


```{r flag_daily_mean}
# create df of sites and their codes for data flag, but on a daily timescale
flag_daily_values<- flag_values %>%
  mutate(start_date = as.Date(start_dt), 
         end_date = as.Date(end_dt)) %>%
  select(site, FLAG, start_date, end_date, notes)

##### Daily flag function in `apply_daily_flag.R`

# this may induce duplicates!
correct_and_flagged_daily_df <- pmap_dfr(flag_daily_values, apply_flag_daily)

# plot to make sure everything looks correct and that no data is missing. 
sample_plot <- filter(correct_and_flagged_daily_df, site_code == "SAWM") %>%
  ggplot() +
  geom_point(aes(x = date, y = mean_corrected_stage_cm, color = flag_type  )) +
  #geom_line(aes(x = date, y = mean_sensor_stage_cm))+
  scale_color_manual(values = flag_colors) +
   theme_bw(base_size = 24)+
  ylab("Daily Mean Stage (cm)")

ggplotly(sample_plot)

```

## *Exporting corrected daily means data*

```{r export_daily}
# write daily averages file matched up with samples
final_daily_means_stage <- correct_and_flagged_daily_df %>%
  select(date, 
         site_code,
         mean_sensor_stage_cm,
         mean_adjusted_stage_cm,
         mean_corrected_stage_cm,
         flag_type, 
         note)

write_csv(final_daily_means_stage, 
          "data/Final/final_daily_means_stage.csv")
```

# *Calculating Q*

*This section contains two ways to calculate rating curves:*

1.  *For data sets where there are few measurements, the function Q \~ (a\*H)\^b.*

2.  *For data sets with more Q/stage measurements, the package `bdrc` may be a more accurate solution. This package calculates more error guidelines but it takes a while to run.*

### *Playing with rating curve package `bdrc`*

*FOR LARGER DATASETS USE THIS PACKAGE. NEEDS FUNCTION DEVELOPMENT TO MAP THROUGH MULTIPLE SITES. THE CODE IN BELOW CHUNK IS USING AN EXAMPLE DATASET PROVIDED WITH THE PACKAGE.*

```{r bdrc}

# library("bdrc")
# 
# data(krokfors)
# 
# gplm.fit <- gplm(Q ~ W, krokfors)
# 
# summary(gplm.fit)
# 
# plot(gplm.fit)

```

## *Functions for Q calculations*
Sourced from "scripts/cap_rod_functions/Q_calcs.R"

1.  *create_rating_curve*

2.  *calc_Q_for_sensor_stage_15min*

3.  *calc_Q_for_sensor_stage_daily*

4.  *EXTRAPOLATION_Q_sensor_15min*

5.  *EXTRAPOLATION_Q_sensor_daily*


## *Doing Q calculations*

Reading in manual measurements and bringing in final_stage_df

Steps:

1.  Run through all manual measurements to create a rating curve model and save coefficients

2.  Double check that measured and modeled Q are in acceptable ranges

    1.  RERUN WITHOUT CERTAIN MEASUREMENTS IF NECESSARY

    2.  USE BEST JUDGEMENT HERE

3.  Run modeled rating curves over sensor stage at 15 min interval and daily interval

    1.  Q IS BOUNDED AT UPPER AND LOWER MANUAL MEASUREMENTS.

    2.  IF Q IS DESIRED TO BE EXTRAPOLATED, USE EXTRAPOLATION_Q_sensor_15min and EXTRAPOLATION_Q_sensor_daily (**not recommended**)

## Manual stage/Q import, cleaning and model creation

### Adjustments if site was moved mid season
```{r}
#########################
# if site was split, use this section
# change_name_manual <-function(old_name, new_name, start_date, end_date) {
#  
#   renamed_manual_df <- manual_h_q %>%
#     dplyr:: filter(site_code == old_name & between(DT, start_date, end_date) ) %>%
#     mutate(site_code = new_name)
#   
#   return(renamed_manual_df)
#   
# }
# 
# # unchanged sites
# manual_h_q_nochange = manual_h_q[!(manual_h_q$site_code %in% 
#                                      unique(rename_df$old_name)),]
# 
# # fix and combine manual H/Q df
# h_q_df <- pmap_dfr(rename_df, change_name_manual) %>%
#   rbind(manual_h_q_nochange) %>%
#   select(H = H_cm, Q = Q_cfs, site_code )
###############################
```

## Creating rating curves

```{r rating_curve_model}
#Import csv with measurement DT, stage H and measured Q
manual_h_q <- manual_measurements %>%
  filter(!is.na(Q_cfs))
# 2022 measurements
manual_h_q_2022 <- read_csv("data/raw/manual_stage_2022.csv", show_col_types = FALSE)%>%
  mutate(DT = mdy_hm(DT, tz = "MST"))%>%
  filter(!is.na(Q_cfs))

h_q_df <- manual_h_q %>%
  bind_rows(manual_h_q_2022)%>%
  mutate(Year = as.character(year(DT)))%>%
 select(H_cm,Q_cfs,site_code, Year, DT)

# list sites that we want to calculate rating curve for
site_list <- tibble(site = c("SAWM"))

# using manual dataframe to create coefficients for each site
manual_H_Q_rc <- pmap_dfr(site_list, create_rating_curve)

# double check that modelled Q matches measured Q
double_check_model <- manual_H_Q_rc %>%
  ggplot() +
  geom_point(aes(x = H_cm, y = Q_cfs, color = Year)) +
  geom_point(aes(x = H_cm, y = calc_Q_cfs), color = "red") +
  geom_line(aes(x = H_cm, y = calc_Q_cfs), color = "green") +
  theme_bw() +
  facet_wrap(~site_code)

plot(double_check_model)  

r2_values <- dplyr::select(manual_H_Q_rc, c(site_code, r2))%>%
  distinct(site_code, .keep_all = TRUE)
head(r2_values)
```

## *15 min Q calculations*

```{r 15min_Q}


# calculate bounded Q for 15min data for all sites in site_list
stage_Q_15min <- pmap_dfr(site_list, calc_Q_for_sensor_stage_15min)

# view indv sites in plot
double_check_stage_Q <- filter(stage_Q_15min, site_code == "SAWM") %>%
  ggplot() +
  geom_line(aes(x = DT, y = Q_cfs, group = site_code, 
                #color = flag_type
                )) +
  geom_point(data = manual_H_Q_rc, aes(x = DT, y = Q_cfs), color = "red") +
  #scale_color_manual(values = flag_colors) +
  theme_bw()

double_check_stage <- filter(stage_Q_15min, site_code == "SAWM") %>%
  ggplot() +
  geom_line(aes(x = DT, y = corrected_stage_cm, group = site_code, 
                #color = flag_type
                )) +
  geom_point(data = manual_H_Q_rc, aes(x = DT, y = H_cm), color = "red") +
  #scale_color_manual(values = flag_colors) +
  theme_bw()

ggarrange(double_check_stage_Q, double_check_stage)
plot(double_check_stage)

### EXTRAPOLATION DANGER ZONE ###
# USE EXTRAPOLATED AT OWN RISK, ERROR MAY BE PROPAGATED
EXTRAPOLATED_stage_Q_15min <- pmap_dfr(site_list, EXTRAPOLATION_Q_sensor_15min)

extrapolated_stage_Q <-
  ggplot() +
  geom_line(data = filter(EXTRAPOLATED_stage_Q_15min, site_code == "SAWM"),
            aes(x = DT, y = Q_cfs), color = "blue") +
  geom_line(data = filter(stage_Q_15min , site_code == "SAWM"), 
            aes(x = DT, y = Q_cfs), color = "red") +
  scale_color_manual(values = flag_colors)+
  theme_bw()

plot(extrapolated_stage_Q)
```

## *Daily Q calculations*

```{r dailyQ}


# calculate bounded Q for daily data for all sites in site_list
stage_Q_daily <- pmap_dfr(site_list, calc_Q_for_sensor_stage_daily)

# view all sites in plotly
double_check_stage_Q <- stage_Q_daily %>%
  ggplot() +
  geom_line(aes(x = date, y = Q_cfs, group = site_code, color = site_code)) +
  # scale_color_manual(values = flag_colors) +
  theme_bw()

ggplotly(double_check_stage_Q)

### EXTRAPOLATION DANGER ZONE ###
# USE EXTRAPOLATED AT OWN RISK, ERROR MAY BE PROPAGATED

EXTRAPOLATED_stage_Q_daily <- pmap_dfr(site_list, EXTRAPOLATION_Q_sensor_daily)

extrapolated_stage_Q <-
  ggplot() +
  geom_line(data = filter(EXTRAPOLATED_stage_Q_daily, site_code == "SAWM"),
            aes(x = date, y = Q_cfs), color = "red") +
  geom_line(data = filter(stage_Q_daily, site_code == "SAWM"),
            aes(x = date, y = Q_cfs), color = "blue") +
  
  # scale_color_manual(values = flag_colors) +
  theme_bw()

ggplotly(extrapolated_stage_Q)
```

# *Exporting Q files*

*Save into folder "data/Provisional_Q_stage_data"*

```{r exporting_Q}
folder <- "data/FINAL/"

write_csv(stage_Q_15min, 
          paste0(folder, "stage_Q_15min_CPF2023.csv"))

write_csv(stage_Q_daily, 
          paste0(folder, "stage_Q_15min_CPF2023.csv"))

write_csv(EXTRAPOLATED_stage_Q_15min, 
          paste0(folder, "EXTRAPOLATED_stage_Q_15min_CPF2023.csv"))

write_csv(EXTRAPOLATED_stage_Q_daily, 
          paste0(folder, "EXTRAPOLATED_stage_Q_daily_CPF2023.csv"))
```

# Clean up
```{r}
# Assuming you want to keep variables a, b, and c
variables_to_keep <- c("manual_H_Q_rc", "stage_Q_15min", "stage_Q_daily")

# Get a list of all objects in the environment
all_objects <- ls()

# Identify the objects to remove
objects_to_remove <- setdiff(all_objects, variables_to_keep)

# Remove the identified objects
rm(list = objects_to_remove)
rm(all_objects, objects_to_remove)



```

# *End*
