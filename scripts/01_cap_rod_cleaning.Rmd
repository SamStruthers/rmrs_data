---
title: "Caprod Cleaning"
format: html
editor: visual
author: "Sam Struthers"
author-meta: "Sam Struthers"
---

## *Packages*

```{r setup}

#change to whatever your working directory is 
#opts_knit$set(root.dir = "~/Repositories/cameron_peak_fire_study/")
source("scripts/00_setup.R")
walk(list.files(path = "scripts/cap_rod_functions/", full.names = TRUE), source)
```



## *Importing and Combining*

*RULES:*

1.  *All data must be in CSVs nested in the folders data/all_sites/*

2.  *All files must have naming convention: SITE\_#DATE#*

3.  *All files must have same number of columns and column names*

4.  *All files must have the same DateTime format (can be adjusted below)*

5.  *Run all_data first to get what the pattern for gsub will be*

    1.  *This is also a good way to test that none of your files are broken*

6.  *Use plots to double check everything*

    1.  *Use the plots to double check there are no zeros near site visits. If there are, remove them from the dataset manually! This will help the adjustments run correctly.*

7.  *Manual Measurements need the columns:*

    1.  *first_measurement_DT: 1st site visit datetime*

        1.  *In same tz as sensor data*

    2.  *last_measurement_DT: 2nd site visit*

        1.  *This becomes first_measurement_DT in next column*

    3.  *first_measurement_cm: 1st site visit stage measurement*

        1.  *cm or whatever units you want the sensor in*

    4.  *last_measurement_cm: 2nd site visit stage measurement*

    5.  *optional columns : Q_cfs and notes*

        1.  *Q_cfs is read in a different chunk*

```{r import_sensor_manual}
# location of raw data files
folder_path <- "data/raw/cap_rod/"


import_cap_rod <- function(folder_path){
  #get all the filenames in the folder
 filenames <-  list.files(folder_path, recursive = TRUE, full.names = TRUE)
 
 grab_data <- function(file_name){
   #grab site name from filename
    file_parts <- strsplit(file_name, "/")[[1]]
  site <- strsplit(file_parts[5], "_")[[1]][1]
  #read in data
  stage_df <- read_xlsx(file_name)%>%
  #add site name to dataset
    mutate(site_code = site, 
           #convert datetime to DT object for R
           # will only work in some formats
           DT_1stpass = parse_date_time(datetime, orders = c("dmY HMS","dmy HMS",  "mdY HM"), tz = "MST" ), 
           #some dates are still numeric due to excel error
           #correcting those here with an ifelse (case_when)
           DT_final = case_when(is.na(DT_1stpass) ~ with_tz(as.POSIXct((as.numeric(datetime) * 86400), origin = "1899-12-30 6:30:01",  tz = "UTC"), tzone = "MST"), 
           #if it is not NA (parsed correctly in first pass, grab value from 1st pass)
           TRUE  ~ DT_1stpass))
 }
all_stage <- map_dfr(filenames, grab_data)
 
 
 return(all_stage)
}


tester <- ggplot(filter(all_stage, site_code == "SAWM"), aes(x = DT_final, y = wtemp_p_1))+
  geom_point()
# combine all sites into one data frame:
stage_df <- read_xlsx() 
%>%
  mutate(site_code = substr(path, nchar(path) - 17 + 1, nchar(path)),
         site_code = substr(site_code, 1, 4),
         DT =  as.POSIXct(datetime, format = "%m/%d/%Y %H:%M", tz = "MST")) %>%
  select(DT, site_code, wtrhgt__5) %>%
  # mutate column to correct units and name
  mutate(water_height_cm = wtrhgt__5 /10, 
        time =  format(DT, format = "%H:%M"), 
        # in case excess chars for site_code remain
        site_code = str_extract(string = site_code, 
                                      pattern = "[A-Z]+")) %>%
  # rename raw mm data
  dplyr ::rename(water_height_mm = wtrhgt__5)
  
##  FIELD STAGE MEASUREMENTS. SEE RULES FOR COLUMNS NAMES ##
# change "field stage" to wherever manual values are stored:
field_stage <- "data/flow_rmrs/manual_values_dec_2022.csv"

manual_measurements <- read.csv(field_stage) %>%
  mutate(first_measurement_DT = as.POSIXct(first_measurement_DT, 
                                           # adjust DT formatting if necessary
                                           format = "%m/%d/%Y %H:%M", tz = "MST"), 
         last_measurement_DT = as.POSIXct(last_measurement_DT, 
                                          format = "%m/%d/%Y %H:%M", tz = "MST"))
```

## *Visual*

-   *Check for missing data, weird dates, see if there are any 0s near site visits.*

```{r pre_clean_visual}
master_visual <- stage_df %>%
  ggplot(aes(x = DT, y = water_height_cm, color = site_code)) +
  geom_line()+
  geom_point(data = manual_measurements, aes(x = first_measurement_DT, 
                                                  y = first_measurement_cm, color = site_code)) +
  theme_bw()
plot(master_visual)

indv_visual <- filter(stage_df, site_code == "bl4") %>%
  ggplot(aes(x = DT, y = water_height_cm, color = site_code)) +
  geom_line()+
  geom_point(data = filter(manual_measurements, site_code == "bl4"), aes(x = first_measurement_DT, 
                                                  y = first_measurement_cm),
             color = "blue") +
  theme_bw()
   
ggplotly(indv_visual)
```

## *Export uncleaned stage across all sites*

```{r compiled_export}
write_csv(stage_df, "data/flow/flow_rmrs/Provisional_Q_stage_data/stage_combined_UNCLEAN.csv")
```

# *Field data adjustments*

*In broad strokes, these functions/loops will take the sensor data pulled in above and manual values measured in the field to correct sensor data to match field data. Workflow overview:*

1.  *Subset the sensor data frame by each field measurement (i.e., each subset is bounded by two field measurements).*
2.  *Step sensor reading up or down based on the difference between first field measurement and the sensor value.*
3.  *Create a new column, time factor, for each subset of data which allows the `driftR` package to correct for sensor drift.*
4.  *Use `driftR` functions to match up the end of the data set with last field measurement.*

## Testing Section:

```{r}


indv_visual <- bl4_clean_nice  %>%
  ggplot(aes(x = DT)) +
  geom_point(aes(y = p1, color = flag_sd))+
  scale_color_manual(values = c( "TRUE"="cyan", "FALSE" = "black" ))+
  geom_point(data = filter(manual_measurements, site_code == "bl4"),
             aes(x = first_measurement_DT, 
                y = first_measurement_cm),
             color = "purple") +
  theme_bw()
   



```

## *Functions to subset and adjust stage values*

```{r subset_adjust_functions}
# functions in subset_and_adjust.R
```

## *Preforming the adjustments*

*Using the functions written above and pmap, we can map over the manual measurements df and preform the adjustments.*

```{r adjusting}
# fit column names to match argument names in functions
manual_measurements_for_func <- manual_measurements %>% 
  dplyr::select(site_name = site_code, 
         start_date = first_measurement_DT,
         end_date = last_measurement_DT,
         manual_start = first_measurement_cm, 
         manual_end = last_measurement_cm) %>%
  # NA values will break the functions, get rid of em!
  na.omit() %>%
  # pmap_dfr needs tibbles :)
  as_tibble()

##PMAP FOR ADJUSTMENT FUNCTIONS!##

# steps df for each subset between site visits to match the start measurement
stepped_df <- pmap_dfr(manual_measurements_for_func, adjust_stage)

# creates a time factor for each subset using the `driftR` factor function
factor_df <- pmap_dfr(manual_measurements_for_func, dr_factor_subset)

# using factor and last 4 measurements from manual end, corrects for drift so that the manual end and the end measurement match up
corrected_df <- pmap_dfr(manual_measurements_for_func, dr_correctOne_subset)
```

### *Graphing to check work*

```{r check_adjust}
# change site names to whatever site you want to look at
indv_corrected_df <- filter(corrected_df, 
                            site_code == "SAWM")

indv_manual_measurements <- filter(manual_measurements_for_func, 
                                   site_name == "SAWM")
 
  
 indv_corrected_graph <- ggplot() +
  geom_line(data = indv_corrected_df, aes(x = DT , y = water_height_cm), 
            color = "red") +
  geom_line(data = indv_corrected_df, aes(x = DT , y = corrected_stage_cm), 
            color = "black") +
  geom_line(data = indv_corrected_df, aes(x = DT , y = adjusted_stage), 
            color = "green", alpha = .5) +
  geom_point(data = indv_manual_measurements, aes(x = start_date, 
                                                  y = manual_start),
             color = "blue") +
   theme_bw()
 
#ggplotly(indv_corrected_graph)
# ^Interactive graph^
 
plot(indv_corrected_graph)


```

Kampf Version

```{r kampf_check_adjust}

# change site names to whatever site you want to look at
indv_corrected_df <- filter(corrected_df, 
                            site_code == "bl4")

indv_manual_measurements <- filter(manual_measurements_for_func, 
                                   site_name == "bl4")
 
  
 indv_corrected_graph <- ggplot() +
  geom_line(data = indv_corrected_df, aes(x = DT , y = water_height_cm), 
            color = "red") +
  geom_line(data = indv_corrected_df, aes(x = DT , y = corrected_stage_cm), 
            color = "black") +
  geom_line(data = indv_corrected_df, aes(x = DT , y = adjusted_stage), 
            color = "green", alpha = .5) +
  geom_point(data = indv_manual_measurements, aes(x = start_date, 
                                                  y = manual_start),
             color = "blue") +
   theme_bw()
 
#ggplotly(indv_corrected_graph)
# ^Interactive graph^
 
plot(indv_corrected_graph)

#ggsave("output/greyrock corrected.jpg", width = 16, height = 10)

```

## *Splitting sites where caprod was moved*

*LBEA and SHEP had to be moved mid-season so they are now being split into LBEA 1/2 and SHEP 1/2*

*This section can be skipped if sensors do not move locations.*

```{r site_split}

# This dataframe contains the old names, new names and start/end dates for the name changes
rename_df <- data.frame(old_name = c("LBEA", "LBEA", "SHEP","SHEP"), 
                       new_name = c("LBEA1", "LBEA2", "SHEP1", "SHEP2"), 
                       start_date = c("2022-05-01 12:00", "2022-08-19 10:15", 
                                      "2022-05-01 12:00", "2022-08-19 12:30"), 
                       end_date= c("2022-08-19 10:00", "2022-11-01 12:00",
                                   "2022-08-17 15:15", "2022-11-01 12:00")) %>% mutate(start_date = as.POSIXct(start_date, format = "%Y-%m-%d %H:%M", tz = "MST"),
       end_date = as.POSIXct(end_date, format = "%Y-%m-%d %H:%M", tz = "MST"))

##################
# If there are more moves in the future, this can be written in as a csv with the same  column names and read in using the hashed out code below:


# rename_df <- read.csv("FILE_PATH_TO.CSV") %>% 
#mutate(start_date = as.POSIXct(start_date, format = "%Y-%m-%d %H:%M", tz = "MST"),
#       end_date = as.POSIXct(end_date, format = "%Y-%m-%d %H:%M", tz = "MST"))
##################

# function to change site_code to new names in rename_df if between start/end dates
change_site_names <- function(old_name, new_name, start_date, end_date) {
  
  test_function <- corrected_df %>%
    dplyr:: filter(site_code == old_name & between(DT, start_date, end_date)) %>%
    mutate(site_code = new_name)
  
  return(test_function)
  
}

# rest of the sites with no changes to location
non_name_changed_df = corrected_df[!(corrected_df$site_code %in% 
                                       unique(rename_df$old_name)),]

# change site names
corrected_df<- pmap_dfr(rename_df, change_site_names) %>%
  # combine with unchanged df
  rbind(non_name_changed_df)

# check that all the new names are in the df and old names are removed
unique(corrected_df$site_code)

# if old names persist you may need to change the start/end dates so that all rows are included
```

## *Adding flags to 15 min*

*Flags were determined visually and compared to field notes about the streams.*

*Flag names:*

-   *PASS: Data looks good*

-   *FAIL: Data is questionable, caution using it*

-   *CHANNEL: Channel changes occurred, stage may not be accurate*

-   *TREND: Magnitude of data is correct but data trends un-intuitively*

-   *VARIATION: Daily variation is questionable, use larger time step for safety*

-   *Any additional: Explained in notes*

```{r flagging}
# 4 columns:
# start_dt <- when flag starts
# end_dt <- when flag ends
      ## make sure start != end, will duplicate data
# FLAG: flag code
# notes: any notes about the flag code or field notes
flag_values<- read.csv("data/flag_values_caprods_2022.csv") %>%
  mutate(start_dt = as.POSIXct(start_dt, format = "%m/%d/%Y %H:%M", tz = "MST"),
         end_dt = as.POSIXct(end_dt, format = "%m/%d/%Y %H:%M", tz = "MST"))


# apply flag in add flags.R
#map over flag values df
correct_and_flagged_df <- pmap_dfr(flag_values, apply_flag_15min)

# colors to visualize site flags in plots
flag_colors <- c("PASS" = "green", 
                 "FAIL" = "red", 
                 "TREND" = "purple",
                 "VARIATION"= "blue",
                 "CHANNEL"= "orange") 

indv_site <- correct_and_flagged_df %>%
  filter(site_code == "BLAK")

indv_site_flags <- ggplot() +
  geom_point(data = indv_site, aes(x = DT, y = corrected_stage_cm,
                                   color = flag_type )) +
  scale_color_manual(values = flag_colors) +
  theme_bw()
                     
plot(indv_site_flags)                     
```

### *Exporting 15 min stage data*

```{r export_15}
final_stage_df<- correct_and_flagged_df %>%
  select(DT, 
         site_code, 
         sensor_stage_mm = water_height_mm, 
         sensor_stage_cm = water_height_cm, 
         adjustment_cm = adjustment,
         adjusted_stage_cm = adjusted_stage,
         time_correction_value = corrFac, 
         corrected_stage_cm,
         flag_type, 
         notes = note,
         date, 
         time)

write.csv(final_stage_df,
          "data/Provisional_Q_stage_data/corrected_15min_stage_CPF_2022.csv")

```

## *Computing daily averages*

```{r daily_mean}
# take mean for each created stage value
daily_means_stage <- corrected_df %>%
  dplyr::select(water_height_cm, corrected_stage_cm, adjusted_stage, date, site_code) %>%
  group_by(date, site_code) %>%
  dplyr:: summarise(mean_sensor_stage_cm = mean(water_height_cm),
         mean_adjusted_stage_cm = mean(adjusted_stage),
         mean_corrected_stage_cm = mean(corrected_stage_cm)) %>%
  ungroup()%>%
  ## IGNORE UNLESS YOU NEED TO MATCH GRAB SAMPLES UP TO SITES
  mutate(site_label = ifelse(site_code %in% c("LBEA1", "LBEA2"), "LBEA",
                      ifelse(site_code %in% c("SHEP1", "SHEP2"), "SHEP", 
                             site_code)))
  ## HASH OUT ABOVE IF NOT MATCHING SAMPLES AND SITES

# match up with FCW and ISCO Samples 
site_labels <- c("FISH", "BEAV", "BENN", "SAWM", "PENN", "BLAK", "SHEP",
                 "LBEA", "ROAR", "SAWM_ISCO", "FISH_ISCO", "LBEA_ISCO", 
                 'BLAK_ISCO', "LBEAISCO", "BLAKISCO", "SAWMISCO", "FISHISCO" )

# filter RMRS chemistry for sites where stage is measured
cam_pk_chem <- read.csv("data/CamPkChem.csv") %>%
  filter(Era == "FCW" | Era == "ISCO") %>%
  dplyr::rename(site_label = SiteLabel) %>%
  filter(site_label %in% site_labels) %>%
  mutate( date = as.Date(Date, format = "%d-%b-%y"), 
          Year = year(date), 
          dayofyear = yday(date)) %>%
  filter(Year >= 2022) %>%
  filter(SampleType %in% c("NORM", "ISCO")) %>%
  select(date, site_label, Era, EraSamp, dayofyear, SampleType) %>%
mutate(site_label=ifelse(site_label %in% c('FISH','FISH_ISCO'),"FISH",
                  ifelse(site_label %in% c('LBEA','LBEA_ISCO', "LBEAISCO"),"LBEA",
                  ifelse(site_label %in% c('SAWM','SAWM_ISCO'),"SAWM",
                  ifelse(site_label %in% c('BLAK','BLAK_ISCO'),"BLAK", site_label
                         )))))

# group into sites and sample type, 
# get number of samples per date
sample_by_day <- cam_pk_chem %>%
  group_by(site_label, date, SampleType) %>%
  dplyr::summarise(count = n()) %>%
  ungroup()

# create df of sites and their codes for data flag, but on a daily timescale
flag_daily_values<- flag_values %>%
  mutate(start_date = as.Date(start_dt), 
         end_date = as.Date(end_dt)) %>%
  select(site, FLAG, start_date, end_date, notes)

##### Daily flag function in `apply_daily_flag.R`

# this may induce duplicates!
correct_and_flagged_daily_df <- pmap_dfr(flag_daily_values, apply_flag_daily)

# join samples by date and stage data frames
daily_means_stage_w_samples <- correct_and_flagged_daily_df %>%
  left_join(select(sample_by_day, 
                   c(site_label, date, count, SampleType)), 
            by = c("site_label", "date"))

# plot to make sure everything looks correct and that no data is missing. 
sample_plot <- filter(daily_means_stage_stage_w_samples, site_code == "SAWM") %>%
  ggplot() +
  geom_line(aes(x = date, y = mean_corrected_stage_cm, group = site_code, 
                color = flag_type)) +
  geom_line(aes(x = date, y = mean_sensor_stage_cm, group = site_code, 
                #color = flag_type
                ))+
  scale_color_manual(values = flag_colors) +
  geom_point(aes(x = date, y = count,
                 color = "grey"))+
   theme_bw(base_size = 24)+
  ylab("Daily Mean Stage (cm)")

plot(sample_plot)

```

## *Exporting corrected daily means data*

```{r export_daily}
# write daily averages file matched up with samples
final_daily_means_stage <- daily_means_stage_w_samples %>%
  select(date, 
         site_code,
         mean_sensor_stage_cm,
         mean_adjusted_stage_cm,
         mean_corrected_stage_cm,
         SampleType,
         sampleCount = count,
         flag_type, 
         note, 
         site_label)

write_csv(final_daily_means_stage, 
          "data/Provisional_Q_stage_data/stage_daily_mean_and_samples_cleaned_KAMPF_EXAMPLE.csv")
```

# *Calculating Q*

*This section contains two ways to calculate rating curves:*

1.  *For data sets where there are few measurements, the function Q \~ (a\*H)\^b.*

2.  *For data sets with more Q/stage measurements, the package `bdrc` may be a more accurate solution. This package calculates more error guidelines but it takes a while to run.*

### *Playing with rating curve package `bdrc`*

*FOR LARGER DATASETS USE THIS PACKAGE. NEEDS FUNCTION DEVELOPMENT TO MAP THROUGH MULTIPLE SITES. THE CODE IN BELOW CHUNK IS USING AN EXAMPLE DATASET PROVIDED WITH THE PACKAGE.*

```{r bdrc}

# library("bdrc")
# 
# data(krokfors)
# 
# gplm.fit <- gplm(Q ~ W, krokfors)
# 
# summary(gplm.fit)
# 
# plot(gplm.fit)

```

## *Functions for Q calculations*

1.  *create_rating_curve*

2.  *calc_Q_for_sensor_stage_15min*

3.  *calc_Q_for_sensor_stage_daily*

4.  *EXTRAPOLATION_Q_sensor_15min*

5.  *EXTRAPOLATION_Q_sensor_daily*

```{r }
source("scripts/cap_rod_functions/Q_calcs.R")

```

## *Doing Q calculations*

Reading in manual measurements and bringing in final_stage_df

Steps:

1.  Run through all manual measurements to create a rating curve model and save coefficients

2.  Double check that measured and modeled Q are in acceptable ranges

    1.  RERUN WITHOUT CERTAIN MEASUREMENTS IF NECESSARY

    2.  USE BEST JUDGEMENT HERE

3.  Run modeled rating curves over sensor stage at 15 min interval and daily interval

    1.  Q IS BOUNDED AT UPPER AND LOWER MANUAL MEASUREMENTS.

    2.  IF Q IS DESIRED TO BE EXTRAPOLATED, USE EXTRAPOLATION_Q_sensor_15min and EXTRAPOLATION_Q_sensor_daily (**not recommended**)

## Manual stage/Q import, cleaning and model creation

```{r rating_curve_model}
#Import csv with measurement DT, stage H and measured Q
manual_h_q <- read.csv("data/manual_stage_Q_jan2023.csv") %>%
  mutate(DT = as.POSIXct(DT, format = "%m/%d/%Y %H:%M", tz = "MST")) %>%
  na.omit(Q_cfs)


#########################
# if site was split, use this section
change_name_manual <-function(old_name, new_name, start_date, end_date) {
 
  renamed_manual_df <- manual_h_q %>%
    dplyr:: filter(site_code == old_name & between(DT, start_date, end_date) ) %>%
    mutate(site_code = new_name)
  
  return(renamed_manual_df)
  
}

# unchanged sites
manual_h_q_nochange = manual_h_q[!(manual_h_q$site_code %in% 
                                     unique(rename_df$old_name)),]

# fix and combine manual H/Q df
h_q_df <- pmap_dfr(rename_df, change_name_manual) %>%
  rbind(manual_h_q_nochange) %>%
  select(H = H_cm, Q = Q_cfs, site_code )
###############################


# if sites were not split, run code below
# h_q_df <- manual_h_q %>%
# select(H = H_cm, Q = Q_cfs,site_code)

# list sites that we want to calculate rating curve for
site_list <- data.frame(site = c("FISH", "SAWM", "BLAK", "PENN", 
                                 "BEAV", "BENN", "LBEA1", "SHEP1")) %>%
  as_tibble()



# using manual dataframe to create coefficients for each site
manual_H_Q_rc <- pmap_dfr(site_list, create_rating_curve)

# double check that modelled Q matches measured Q
double_check_model <- manual_H_Q_rc %>%
  ggplot() +
  geom_point(aes(x = H, y = Q), color = "blue") +
  geom_point(aes(x = H, y = Q_cfs), color = "red") +
  geom_line(aes(x = H, y = Q_cfs), color = "green") +
  theme_bw() +
  facet_wrap(~site_code)

plot(double_check_model)  

r2_values <- dplyr::select(manual_H_Q_rc, c(site_code, r2))%>%
  distinct(site_code, .keep_all = TRUE)
head(r2_values)
```

## *15 min Q calculations*

```{r 15min_Q}


# calculate bounded Q for 15min data for all sites in site_list
stage_Q_15min <- pmap_dfr(site_list, calc_Q_for_sensor_stage_15min)

# view indv sites in plot
double_check_stage_Q <- filter(stage_Q_15min, site_code == "SAWM") %>%
  ggplot() +
  geom_line(aes(x = DT, y = Q_cfs, group = site_code, 
                #color = flag_type
                )) +
  #scale_color_manual(values = flag_colors) +
  theme_bw()

plot(double_check_stage_Q)

### EXTRAPOLATION DANGER ZONE ###
# USE EXTRAPOLATED AT OWN RISK, ERROR MAY BE PROPAGATED
EXTRAPOLATED_stage_Q_15min <- pmap_dfr(site_list, EXTRAPOLATION_Q_sensor_15min)

extrapolated_stage_Q <-
  ggplot() +
  geom_line(data = filter(EXTRAPOLATED_stage_Q_15min, site_code == "SAWM"),
            aes(x = DT, y = Q_cfs), color = "blue") +
  geom_line(data = filter(stage_Q_15min , site_code == "SAWM"), 
            aes(x = DT, y = Q_cfs), color = "red") +
  scale_color_manual(values = flag_colors)+
  theme_bw()

plot(extrapolated_stage_Q)
```

## *Daily Q calculations*

```{r dailyQ}


# calculate bounded Q for daily data for all sites in site_list
stage_Q_daily <- pmap_dfr(site_list, calc_Q_for_sensor_stage_daily)

# view all sites in plotly
double_check_stage_Q <- stage_Q_daily %>%
  ggplot() +
  geom_line(aes(x = date, y = Q_cfs, group = site_code, color = site_code)) +
  # scale_color_manual(values = flag_colors) +
  theme_bw()

ggplotly(double_check_stage_Q)

### EXTRAPOLATION DANGER ZONE ###
# USE EXTRAPOLATED AT OWN RISK, ERROR MAY BE PROPAGATED

EXTRAPOLATED_stage_Q_daily <- pmap_dfr(site_list, EXTRAPOLATION_Q_sensor_daily)

extrapolated_stage_Q <-
  ggplot() +
  geom_line(data = filter(EXTRAPOLATED_stage_Q_daily, site_code == "SAWM"),
            aes(x = date, y = Q_cfs), color = "red") +
  geom_line(data = filter(stage_Q_daily, site_code == "SAWM"),
            aes(x = date, y = Q_cfs), color = "blue") +
  
  # scale_color_manual(values = flag_colors) +
  theme_bw()

ggplotly(extrapolated_stage_Q)
```

# *Exporting Q files*

*Save into folder "data/Provisional_Q_stage_data"*

```{r exporting_Q}
folder <- "data/Provisional_Q_stage_data/"

write_csv(stage_Q_15min, 
          paste0(folder, "stage_Q_15min_CPF2022.csv"))

write_csv(stage_Q_daily, 
          paste0(folder, "stage_Q_15min_CPF2022.csv"))

write_csv(EXTRAPOLATED_stage_Q_15min, 
          paste0(folder, "EXTRAPOLATED_stage_Q_15min_CPF2022.csv"))

write_csv(EXTRAPOLATED_stage_Q_daily, 
          paste0(folder, "EXTRAPOLATED_stage_Q_daily_CPF2022.csv"))
```

*End*
